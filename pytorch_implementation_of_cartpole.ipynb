{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pytorch implementation of cartpole.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNubW7UCFKFVEbn4dFPYWxu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Arya-Raj/Cartpole-problem-using-Pytorch-and-RL-algorithms/blob/main/pytorch_implementation_of_cartpole.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAu95nIv04Pp"
      },
      "source": [
        "##Actor-Critic algorithm in Cartpole (A2C)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz6KiiA_AtYr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "063c6e1e-60f4-4be5-c92b-74b64ccdeb8a"
      },
      "source": [
        "import torch\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "max_steps_per_episode = 10000\n",
        "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
        "env.seed(seed)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42]"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y47LbugxpuYt"
      },
      "source": [
        "class Actor_critic(nn.Module):\n",
        "    def __init__(self,num_actions,num_value):\n",
        "        super().__init__()\n",
        "        self.layer1=nn.Linear(4,128)\n",
        "        self.actor=nn.Linear(128,num_actions)\n",
        "        self.critic=nn.Linear(128,num_value)\n",
        "    def forward(self,state):\n",
        "        common=F.relu(self.layer1(state))\n",
        "        actor_prob=F.softmax(self.actor(common),dim=-1)\n",
        "        critic_value=self.critic(common)\n",
        "        \n",
        "        return(actor_prob,critic_value)\n",
        "        \n",
        "       \n",
        "model=Actor_critic(2,1)\n",
        "opt= optim.Adam(model.parameters(), lr=0.01)\n",
        "lr_scheduler = optim.lr_scheduler.ExponentialLR(opt, gamma=1. - 2e-5) "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm_7SaPfkPv-",
        "outputId": "2eba93ad-55ea-46a5-8de0-dca5f928351d"
      },
      "source": [
        "action_prob_hist=[]\n",
        "critic_value_hist=[]\n",
        "reward_hist=[]\n",
        "episode=0\n",
        "running_reward=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    episode_reward=0\n",
        "    state=env.reset()\n",
        "    for i in range(0,10000):\n",
        "        #env.render()\n",
        "        #state=torch.FloatTensor(state).unsqueeze(0)\n",
        "        state=torch.from_numpy(state).float()\n",
        "        actor_prob,critic_value=model(state)\n",
        "\n",
        "        m=Categorical(actor_prob)\n",
        "        action=m.sample()\n",
        "        action_prob_hist.append(m.log_prob(action))\n",
        "        critic_value_hist.append(critic_value)\n",
        "\n",
        "        #one_d_tensor=actor_prob.squeeze(dim=0)\n",
        "        #sampled_prob=random.choice(actor_prob)\n",
        "        #action_prob_hist.append(math.log(sampled_prob))\n",
        "        #action=one_d_tensor.tolist().index(sampled_prob)\n",
        "        #action=actor_prob.tolist().index(sampled_prob)\n",
        "        #critic_value_hist.append(critic_value)\n",
        "\n",
        "        state,reward,done,info=env.step(action.item())\n",
        "        reward_hist.append(reward)\n",
        "        episode_reward+=reward\n",
        "        #print(\"for timestep {} action {}, critic_value {},sampled_prob {},episode_reward {}\".format(i,action,critic_value,sampled_prob,episode_reward))\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    returns=[]\n",
        "    actor_loss=[]\n",
        "    critic_loss=[]\n",
        "    running_reward=0.05*episode_reward+(1-0.05)*running_reward\n",
        "    \n",
        "    ds=0\n",
        "    for r in reward_hist[::-1]:\n",
        "        ds=r+0.99*ds\n",
        "        returns.insert(0,ds)\n",
        "    returns=torch.tensor(returns)\n",
        "    returns=returns-returns.mean()/returns.std()+eps\n",
        "    \n",
        "    #print('running_reward {},returns {}'.format(running_reward,returns))    \n",
        "    #tensaction_prob_hist=torch.tensor(action_prob_hist)\n",
        "    #tenscritic_value_hist=torch.tensor(critic_value_hist)\n",
        "    #tensreturns=torch.tensor(returns)\n",
        "\n",
        "    hist=zip(action_prob_hist,critic_value_hist,returns)\n",
        "    for log_prob,value,ret in hist:\n",
        "        loss=ret-value.item()\n",
        "        actor_loss.append(-log_prob*loss)\n",
        "        critic_loss.append(F.smooth_l1_loss(value, torch.tensor([ret])))\n",
        "    opt.zero_grad()\n",
        "    #sum_loss=torch.tensor(sum(actor_loss)+sum(critic_loss),requires_grad=True,dtype=torch.float64)\n",
        "    loss = torch.stack(actor_loss).sum() + torch.stack(critic_loss).sum()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    \n",
        "    #print(\"loss {} actor_loss {} critic_loss\".format(sum_loss,actor_loss,critic_loss))\n",
        "    \n",
        "    action_prob_hist.clear()\n",
        "    critic_value_hist.clear()\n",
        "    reward_hist.clear()\n",
        "    \n",
        "    episode+=1\n",
        "    \n",
        "    if episode%10==0:\n",
        "        print(\"Episode no {} running_reward {}\".format(episode,running_reward))\n",
        "        \n",
        "\n",
        "    if running_reward>=195:\n",
        "        print(\"Solved at episode {} with reward {}\".format(episode,running_reward))\n",
        "        break\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode no 10 running_reward 9.037832377223241\n",
            "Episode no 20 running_reward 14.305686971200565\n",
            "Episode no 30 running_reward 29.274414309736237\n",
            "Episode no 40 running_reward 44.6505708753231\n",
            "Episode no 50 running_reward 58.098968627992896\n",
            "Episode no 60 running_reward 55.55741588311191\n",
            "Episode no 70 running_reward 75.22276790510894\n",
            "Episode no 80 running_reward 82.93959462367081\n",
            "Episode no 90 running_reward 59.42397709400871\n",
            "Episode no 100 running_reward 45.93302228421416\n",
            "Episode no 110 running_reward 41.51756600592898\n",
            "Episode no 120 running_reward 52.76398120800984\n",
            "Episode no 130 running_reward 81.96509438435643\n",
            "Episode no 140 running_reward 105.63927630636577\n",
            "Episode no 150 running_reward 135.7414091105352\n",
            "Episode no 160 running_reward 146.35459484294566\n",
            "Episode no 170 running_reward 140.18255158494748\n",
            "Episode no 180 running_reward 157.6583409557919\n",
            "Episode no 190 running_reward 163.32190910256608\n",
            "Episode no 200 running_reward 166.69749847175288\n",
            "Episode no 210 running_reward 167.20618716599586\n",
            "Episode no 220 running_reward 174.651424413335\n",
            "Episode no 230 running_reward 184.82287143918742\n",
            "Episode no 240 running_reward 190.91289249907166\n",
            "Episode no 250 running_reward 194.559213068364\n",
            "Solved at episode 252 with reward 195.08968979419848\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvTgGlBiVrFT"
      },
      "source": [
        "##Policy gradient algorithm in Cartpole"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LrWw46PenRa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03063dd0-7251-43fa-bcbe-7bb6f591c5f8"
      },
      "source": [
        "import torch\n",
        "import gym\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "gamma = 0.99  # Discount factor for past rewards\n",
        "max_steps_per_episode = 10000\n",
        "env = gym.make(\"CartPole-v0\")  # Create the environment\n",
        "env.seed(seed)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[42]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5iXbNvU5RIt"
      },
      "source": [
        "class Actor_critic(nn.Module):\n",
        "    def __init__(self,num_actions):\n",
        "        super().__init__()\n",
        "        self.layer1=nn.Linear(4,128)\n",
        "        self.actor=nn.Linear(128,num_actions)\n",
        "      \n",
        "    def forward(self,state):\n",
        "        common=F.relu(self.layer1(state))\n",
        "        actor_prob=F.softmax(self.actor(common),dim=-1)\n",
        "        \n",
        "        \n",
        "        return(actor_prob)\n",
        "        \n",
        "       \n",
        "model=Actor_critic(2)\n",
        "opt= optim.Adam(model.parameters(), lr=0.01)\n",
        "lr_scheduler = optim.lr_scheduler.ExponentialLR(opt, gamma=1. - 2e-5) "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-mMP3hsWT5L",
        "outputId": "fe4fe31d-9e42-4ecc-883e-42512853120a"
      },
      "source": [
        "action_prob_hist=[]\n",
        "reward_hist=[]\n",
        "episode=0\n",
        "running_reward=0\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "while True:\n",
        "    episode_reward=0\n",
        "    state=env.reset()\n",
        "    for i in range(0,10000):\n",
        "        state=torch.from_numpy(state).float()\n",
        "        actor_prob=model(state)\n",
        "\n",
        "        m=Categorical(actor_prob)\n",
        "        action=m.sample()\n",
        "        action_prob_hist.append(m.log_prob(action))\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "        state,reward,done,info=env.step(action.item())\n",
        "        reward_hist.append(reward)\n",
        "        episode_reward+=reward\n",
        "        if done:\n",
        "            break\n",
        "    \n",
        "    returns=[]\n",
        "    actor_loss=[]\n",
        "    running_reward=0.05*episode_reward+(1-0.05)*running_reward\n",
        "    \n",
        "    ds=0\n",
        "    for r in reward_hist[::-1]:\n",
        "        ds=r+0.99*ds\n",
        "        returns.insert(0,ds)\n",
        "    returns=torch.tensor(returns)\n",
        "    returns=returns-returns.mean()/returns.std()+eps\n",
        "    \n",
        "\n",
        "    hist=zip(action_prob_hist,returns)\n",
        "    for log_prob,ret in hist:\n",
        "        actor_loss.append(-log_prob*ret)\n",
        "    opt.zero_grad()\n",
        "    loss = torch.stack(actor_loss).sum()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    \n",
        "    \n",
        "    action_prob_hist.clear()\n",
        "    reward_hist.clear()\n",
        "    \n",
        "    episode+=1\n",
        "    \n",
        "    if episode%10==0:\n",
        "        print(\"Episode no {} running_reward {}\".format(episode,running_reward))\n",
        "        \n",
        "\n",
        "    if running_reward>=195:\n",
        "        print(\"Solved at episode {} with reward {}\".format(episode,running_reward))\n",
        "        break"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode no 10 running_reward 9.720432464886816\n",
            "Episode no 20 running_reward 20.894507145365033\n",
            "Episode no 30 running_reward 36.91340877352152\n",
            "Episode no 40 running_reward 33.96233240013956\n",
            "Episode no 50 running_reward 36.42983999271926\n",
            "Episode no 60 running_reward 49.82747565796303\n",
            "Episode no 70 running_reward 80.96882361591129\n",
            "Episode no 80 running_reward 117.5897973222537\n",
            "Episode no 90 running_reward 121.28246998873071\n",
            "Episode no 100 running_reward 101.704619110349\n",
            "Episode no 110 running_reward 87.26433467600441\n",
            "Episode no 120 running_reward 113.0092700837242\n",
            "Episode no 130 running_reward 124.42349838153926\n",
            "Episode no 140 running_reward 132.58349385855277\n",
            "Episode no 150 running_reward 148.09520610624284\n",
            "Episode no 160 running_reward 158.8141757454579\n",
            "Episode no 170 running_reward 171.12281655363816\n",
            "Episode no 180 running_reward 182.71016356950005\n",
            "Episode no 190 running_reward 189.09793625567016\n",
            "Episode no 200 running_reward 171.01510173485383\n",
            "Episode no 210 running_reward 148.12860862877562\n",
            "Episode no 220 running_reward 149.08901736370206\n",
            "Episode no 230 running_reward 163.93256255043002\n",
            "Episode no 240 running_reward 174.32873945777268\n",
            "Episode no 250 running_reward 159.23188732302103\n",
            "Episode no 260 running_reward 140.7230490575578\n",
            "Episode no 270 running_reward 139.12414266102158\n",
            "Episode no 280 running_reward 151.48301785411684\n",
            "Episode no 290 running_reward 169.09712333116138\n",
            "Episode no 300 running_reward 181.21797129120088\n",
            "Episode no 310 running_reward 188.7545056182062\n",
            "Episode no 320 running_reward 193.26690711362235\n",
            "Solved at episode 326 with reward 195.05055802039888\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Gl_L9MYdvAC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}